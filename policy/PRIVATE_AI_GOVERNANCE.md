# PRIVATE AI GOVERNANCE
## Regulating Concentrated AI Power
### The Human Standard

---

## Executive Summary

The Human Standard focuses extensively on government AI but under-addresses a critical reality: the most powerful AI systems are controlled by a handful of private companies. OpenAI, Anthropic, Google DeepMind, and Meta control the foundational models that underpin most AI applications. This concentration of power requires a governance framework as robust as what we propose for government.

**The Core Principle**: Power requires accountability, whether public or private. AI systems that shape society must be governed democratically, regardless of who owns them.

---

## Part I: The Concentration Problem

### The Landscape (2025)

| Company | Key Models | Estimated Market Power |
|---------|------------|----------------------|
| OpenAI | GPT-4/5, DALL-E | 30-40% of enterprise AI |
| Google DeepMind | Gemini, AlphaFold | 25-35% |
| Anthropic | Claude | 10-15% |
| Meta | Llama series | 10-15% (open weights) |
| Others | Various | 10-20% |

### Why This Matters

**Foundational Model Dependence**:
- Most AI applications are built on a few foundational models
- Smaller companies and governments depend on these providers
- The foundation providers have extraordinary leverage

**Power Asymmetries**:
- These companies know more about AI capabilities than governments
- Regulatory capture is a serious risk
- Traditional antitrust frameworks don't fit AI well

**Risks of Concentration**:
- Single points of failure for critical infrastructure
- Homogenized AI values/biases across society
- Private control of transformative technology
- Extraction of monopoly rents

### Government vs. Private AI Power

We propose extensive governance for government AI. But private AI may be more consequential:

| Dimension | Government AI | Private AI |
|-----------|--------------|------------|
| Scale of impact | Millions (government services) | Billions (commercial applications) |
| Transparency | Mandated (under our framework) | Currently minimal |
| Accountability | Democratic oversight | Shareholder/board only |
| Value alignment | Public interest (in theory) | Profit maximization |

---

## Part II: The Regulatory Framework

### Tier 1: Licensing for Foundational Models

**Threshold**: Any model trained on >$100M compute (or equivalent)

**Requirements for License**:
- Register with National AI Oversight Commission (NAICO)
- Submit to mandatory evaluations before deployment
- Disclose training data sources (summary level)
- Maintain incident reporting system
- Accept audit rights

**Consequences of Non-Compliance**:
- Civil penalties up to $100M per violation
- Prohibition on U.S. market deployment
- Criminal liability for executives in extreme cases

### Tier 2: Safety Standards for All Licensed Models

**Pre-Deployment Requirements**:
- Red team evaluation (adversarial testing)
- Bias and discrimination testing
- Capability evaluations (especially for dangerous capabilities)
- Safety feature verification (refusals, guardrails)

**Ongoing Requirements**:
- Quarterly incident reporting
- Annual third-party safety audit
- Continuous monitoring for emergent behaviors
- Update NAICO on significant capability improvements

**Prohibited Capabilities** (without special authorization):
- Autonomous weapons development
- CBRN (chemical, biological, radiological, nuclear) assistance
- Mass surveillance systems
- Manipulation/deception at scale

### Tier 3: Interoperability and Access Requirements

**For Models with >10M Users**:
- API access at reasonable rates (non-discriminatory pricing)
- Data portability (users can export their data)
- Interoperability with competing services
- No exclusive dealing that forecloses competition

**For Models Deemed "Critical Infrastructure"**:
- Service continuity obligations
- Backup/redundancy requirements
- Government access provisions (for emergency use)
- Price regulation if monopoly power demonstrated

### Tier 4: Liability Framework

**Strict Liability** for:
- Physical harm caused by AI systems
- Discrimination in covered applications (employment, credit, housing)
- Failure to implement required safety features

**Negligence Standard** for:
- Unforeseeable harms despite reasonable precautions
- Third-party misuse of open models

**Safe Harbors** for:
- Good-faith compliance with safety standards
- Open-source models meeting specified criteria
- Research and development (not commercial deployment)

---

## Part III: Structural Interventions

When regulation is insufficient, structural remedies may be necessary.

### Escalation Ladder

| Level | Market Condition | Intervention |
|-------|------------------|--------------|
| **1. Regulation** | Competition exists, concerns about safety/access | Licensing, standards, transparency |
| **2. Interoperability** | Dominant player(s), network effects | Mandated APIs, data portability |
| **3. Behavioral Remedies** | Significant market power | Line-of-business restrictions, non-discrimination |
| **4. Structural Separation** | Vertical integration harm | Separate model development from deployment |
| **5. Breakup** | Intractable monopoly | Divide into competing entities |
| **6. Public Option** | Market failure | Government-funded open alternative |
| **7. Nationalization** | Critical infrastructure, last resort | Public ownership of essential AI |

### Current Recommendation: Levels 1-3

- Implement comprehensive licensing (Level 1)
- Mandate interoperability for dominant providers (Level 2)
- Prohibit certain vertical integration (Level 3)

Escalate to higher levels only if lower levels prove insufficient.

### Specific Structural Concerns

**Vertical Integration Risks**:
| Integration | Risk | Potential Remedy |
|-------------|------|------------------|
| Model + Cloud (AWS/Azure/GCP) | Preferential treatment for own models | Non-discrimination requirements |
| Model + Consumer Products (Google) | Data advantage, foreclosure | Data sharing requirements |
| Model + Operating Systems | Distribution advantage | App store neutrality |
| Model + Search | Information control | Algorithmic transparency |

**Horizontal Concentration Risks**:
- Collusion on safety (slowing deployment to protect incumbents)
- Collusion on pricing
- Talent hoarding
- Patent thickets

---

## Part IV: The Public Option

### Why a Public AI Option?

Even with robust regulation, private AI may:
- Optimize for profit over public interest
- Exclude underserved populations
- Align with corporate rather than human values
- Create dependence on private entities for public functions

A publicly-funded, open-source alternative provides:
- Competitive pressure on private providers
- Fallback if private markets fail
- Model aligned with public interest
- Research platform for safety work

### Implementation: National AI Laboratory

**Structure**:
- Federally funded (~$10B/year)
- Independent governance (insulated from political interference)
- Open-source outputs (weights, code, research)
- Located within NIST or as independent agency

**Mission**:
- Develop open-source foundational models
- Conduct AI safety research
- Provide AI services to government agencies
- Serve as benchmark for private AI safety

**Not Intended To**:
- Replace private AI development
- Compete on commercial applications
- Limit private innovation

**Think**: National Institutes of Health for AI—basic research and public goods that complement private development.

---

## Part V: The National AI Oversight Commission (NAICO)

### Structure

**Commissioners** (7 members):
- 3 appointed by President (confirmed by Senate)
- 2 appointed by congressional leadership (1 each party)
- 2 technical experts (appointed by National Academy of Sciences)
- 7-year staggered terms
- Removal only for cause

**Divisions**:
| Division | Responsibility |
|----------|----------------|
| Licensing | Process applications, monitor compliance |
| Safety | Develop standards, conduct evaluations |
| Competition | Monitor market concentration, recommend remedies |
| Research | Conduct and fund AI governance research |
| International | Coordinate with foreign regulators |

### Powers

- Issue and revoke AI licenses
- Conduct investigations and audits
- Impose civil penalties (up to $100M per violation)
- Refer criminal matters to DOJ
- Issue binding regulations
- Recommend structural remedies to DOJ/FTC

### Independence

- Funded by fees on licensed entities (not appropriations)
- Commissioners removable only for cause
- Required technical expertise for appointees
- Public transparency (meetings, rulemakings)

---

## Part VI: AI Training Data Rights

### The Problem

AI models are trained on data created by humans:
- Writers, artists, programmers, photographers
- Social media posts, forum discussions
- Websites, books, academic papers
- Code repositories, creative works

Creators receive no compensation. Value flows entirely to model developers.

### The Solution: Training Data Royalties

**Mandatory Disclosure**:
- All commercial AI models must disclose training data sources (summary level)
- Specific datasets used
- Categories of web-scraped content

**Opt-In/Opt-Out System**:
- Creators can register to opt-in (with compensation) or opt-out
- Opt-out must be honored (technical and legal enforcement)
- Default: Opt-in with compensation for commercial use

**Royalty Structure**:
| Content Type | Rate |
|--------------|------|
| Text (per 1M tokens trained on) | $100 |
| Images (per 1M images) | $500 |
| Code (per 1M lines) | $200 |
| Audio (per 1M minutes) | $300 |
| Video (per 1M minutes) | $1,000 |

**Collection and Distribution**:
- NAICO collects royalties
- Distributes to registered creators
- Unclaimed royalties fund public domain creation

### Revenue Estimate

- Current AI training uses trillions of tokens, billions of images
- Estimated annual royalties: $5-20B
- Funds flow to creators, not corporations

---

## Part VII: Algorithmic Accountability in Private AI

### High-Risk Applications

Certain AI applications require enhanced accountability regardless of government/private:

| Application | Requirements |
|-------------|--------------|
| Employment decisions | Bias audit, explanation rights, human review |
| Credit decisions | Fair lending compliance, adverse action notices |
| Housing decisions | Fair housing compliance, disparate impact testing |
| Healthcare decisions | Clinical validation, physician oversight |
| Criminal justice | Validation, transparency, appeal rights |
| Insurance | Actuarial justification, non-discrimination |

### Algorithmic Impact Assessments

**Required For**: Any AI system with >1M users in high-risk application

**Assessment Must Include**:
- Purpose and intended use
- Training data sources and potential biases
- Accuracy metrics across demographic groups
- Potential harms and mitigations
- Human oversight mechanisms
- Appeal/redress procedures

**Published**: Summary version publicly available

**Reviewed By**: NAICO or delegated agency

### Explanation Rights

Individuals affected by significant AI decisions have the right to:
- Know that AI was used in the decision
- Receive explanation of key factors
- Request human review
- Access data used about them

Applies to: Employment, credit, housing, healthcare, education, insurance decisions

---

## Part VIII: Open Source and Decentralization

### The Open Source Question

Open source/open weights models present different risks:
- Democratize access (good)
- Can't be recalled once released (risk)
- Enable diverse applications (good)
- Enable misuse by bad actors (risk)

### Our Position

**Support for Open Source** with guardrails:

- Open source models below capability threshold: Full support, no restrictions
- Open source models above threshold: Safety evaluation before release, liability for releases that violate standards
- Open source does not mean unaccountable: Publishers have responsibility

### Safe Harbor for Open Source

Open source developers receive liability safe harbor if:
- Model passes pre-release safety evaluation
- No deliberate dangerous capability inclusion
- Reasonable safety mitigations implemented
- Cooperation with post-release incident investigation

### Balancing Access and Safety

| Model Capability | Recommended Policy |
|-----------------|-------------------|
| Below GPT-3.5 level | Full open source, minimal restrictions |
| GPT-4 level | Open source with safety evaluation |
| Above GPT-4 | Staged release, demonstrated safety |
| AGI-adjacent | Restricted release, extensive evaluation |

---

## Part IX: International Coordination

### The Challenge

AI development is global. U.S. regulation alone is insufficient.

### Strategy

**Bilateral**:
- AI safety agreements with allies (UK, EU, Japan, Australia)
- Mutual recognition of safety standards
- Information sharing on incidents and threats

**Multilateral**:
- International AI safety standards body (like IAEA for nuclear)
- Coordinated approach to high-risk capabilities
- Technology sharing agreements for beneficial AI

**Competitive**:
- Maintain technological leadership through investment
- Prevent regulatory race to the bottom
- Export controls on dangerous capabilities

### China Strategy

- Accept that full cooperation is unlikely
- Pursue narrow cooperation on existential risks (AI safety)
- Maintain technological parity through investment
- Defensive measures against AI-enabled threats

---

## Part X: Implementation Timeline

### Phase 1 (Year 1): Foundation

- Establish NAICO (temporary structure, permanent legislation pending)
- Begin licensing process for existing foundational models
- Develop initial safety standards

### Phase 2 (Years 2-3): Full Regulatory Framework

- Permanent NAICO authorization
- Comprehensive licensing operational
- Interoperability requirements for dominant providers
- Training data royalty system launched

### Phase 3 (Years 3-5): Mature Governance

- Full safety evaluation regime
- Public AI option operational
- International coordination structures
- Structural remedies if needed

---

## Conclusion

Private AI power is at least as consequential as government AI power. The same principles that demand government AI accountability—transparency, democratic control, human oversight—apply to private AI.

We do not seek to halt AI development. We seek to ensure it serves humanity rather than a narrow set of shareholders. The companies building these systems have done remarkable work. They also have unprecedented power that requires democratic governance.

The alternative—allowing a handful of corporations to shape humanity's future without accountability—is unacceptable. The Human Standard applies to all powerful AI, regardless of who owns it.

---

*"Power requires accountability. The most consequential technology in human history cannot be governed by shareholder meetings and terms of service."*

—The Human Standard
